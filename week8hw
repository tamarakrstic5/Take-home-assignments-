import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, auc
import matplotlib.pyplot as plt
glass = pd.read_csv('glass.csv')
glass['household'] = glass.Type.map({1:0, 2:0, 3:0, 5:1, 6:1, 7:1})
glass.sort_values( by = 'Al', inplace=True)
X= np.array(glass.Al).reshape(-1,1)
y = glass.household
logreg = LogisticRegression()
logreg.fit(X,y)
pred = logreg.predict(X)
logreg.coef_, logreg.intercept_
y_prob = logreg.predict_proba(X_test)[:, 1]
y_pred_default = (y_prob >= 0.5).astype(int)
accuracy_default = accuracy_score(y_test, y_pred_default)
precision_default = precision_score(y_test, y_pred_default)
recall_default = recall_score(y_test, y_pred_default)
thresholds = [0.3, 0.4, 0.6, 0.7]
results = []

for threshold in thresholds:
    y_pred = (y_prob >= threshold).astype(int)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    results.append((threshold, accuracy, precision, recall))
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray' )
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.show()






import pandas as pd
import numpy as np
from sklearn.metrics import pairwise_distances
from sklearn import cluster, datasets, preprocessing, metrics
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('fivethirtyeight')
X_scaled = preprocessing.MinMaxScaler().fit_transform(df[cols])k = 1
kmeans = cluster.KMeans(n_clusters=k)
kmeans.fit(X_scaled)
metrics.silhouette_score(X_scaled, labels, metric='euclidean')
score=[]
for k in range(0,10):
  kmeans = cluster.KMeans(n_clusters=k)
  kmeans.fit(X_scaled)
  labels = kmeans.labels_
  centroids = kmeans.cluster_centers_
  inertia = kmeans.inertia_
  score.append(metrics.silhouette_score(X_scaled, labels, metric='euclidean'))
plt.plot(list(range(2,20)),score)
plt.xlabel('k')
plt.ylabel('Score')
inertia_score=[]
for k in range(0,10):
  kmeans = cluster.KMeans(n_clusters=k)
  kmeans.fit(X_scaled)
  labels = kmeans.labels_
  centroids = kmeans.cluster_centers_
  inertia = kmeans.inertia_
  inertia_score.append(inertia)
plt.plot(list(range(2,20)),inertia_score)
plt.xlabel('k')
plt.ylabel('Inertia')
Inertia tends to decrease as k increases. However, a very low inertia does not necessarily mean the clusters are meaningful. A small k might group data too broadly, while a very large k might lead to overfitting by creating too many clusters that don’t generalize well.
Silhouette Score provides a measure of how well-defined the clusters are. A higher silhouette score indicates better-defined clusters, but it’s not always guaranteed to reach a peak at a single k. The silhouette score can guide the choice of k, but the choice might depend on practical constraints or the specific domain you are working with.
Sometimes, the "right" k can be determined by visualizing the clusters and inspecting the separability.
One of the common techniques for choosing k is the Elbow Method, where you plot the inertia for different values of k and look for an "elbow" point where the rate of decrease in inertia slows down.The point where the curve flattens (the "elbow") indicates the optimal value of k, balancing between cluster purity and complexity.

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
df = pd.read_csv('food_nutrients.csv')
df_nutrients = df.select_dtypes(include=[np.number])
df_nutrients = df_nutrients.fillna(df_nutrients.median())
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_nutrients)
k_values = range(2, 11)
inertia_scores = []
silhouette_scores = []
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(df_scaled)
inertia_scores.append(kmeans.inertia_)  
    silhouette_avg = silhouette_score(df_scaled, kmeans.labels_)  
    silhouette_scores.append(silhouette_avg)
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(k_values, inertia_scores, marker='o', color='blue')
plt.title('Inertia for Different k Values')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.subplot(1, 2, 2)
plt.plot(k_values, silhouette_scores, marker='o', color='red')
plt.title('Silhouette Score for Different k Values')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.tight_layout()
plt.show()
kmeans_no_scale = KMeans(n_clusters=4, random_state=42)
kmeans_no_scale.fit(df_nutrients)
inertia_no_scale = kmeans_no_scale.inertia_
silhouette_no_scale = silhouette_score(df_nutrients, kmeans_no_scale.labels_)
kmeans_scaled = KMeans(n_clusters=4, random_state=42)
kmeans_scaled.fit(df_scaled)
inertia_scaled = kmeans_scaled.inertia_
silhouette_scaled = silhouette_score(df_scaled, kmeans_scaled.labels_)
print(f"Inertia without scaling: {inertia_no_scale}")
print(f"Silhouette Score without scaling: {silhouette_no_scale}")
print(f"Inertia with scaling: {inertia_scaled}")
print(f"Silhouette Score with scaling: {silhouette_scaled}")
plt.plot(k_values, inertia_scores, marker='o', color='blue')
plt.title('Elbow Method: Inertia vs k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.show()

